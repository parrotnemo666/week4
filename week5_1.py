"""
Task 1: ç¥ç¶“ç¶²è·¯åå‘å‚³æ’­ - è¿´æ­¸ä»»å‹™
ä½¿ç”¨ MSE æå¤±å‡½æ•¸
"""

import numpy as np


# ============================================
# æ¿€æ´»å‡½æ•¸åŠå…¶å°æ•¸
# ============================================

class ReLU:
    """ReLU æ¿€æ´»å‡½æ•¸"""
    
    @staticmethod
    def forward(x):
        """å‰å‘å‚³æ’­ï¼šReLU(x) = max(0, x)"""
        return np.maximum(0, x)
    
    @staticmethod
    def derivative(x):
        """è¨ˆç®—å°æ•¸ï¼šx > 0 æ™‚ç‚º 1ï¼Œå¦å‰‡ç‚º 0"""
        return (x > 0).astype(float)


class Linear:
    """Linear æ¿€æ´»å‡½æ•¸ï¼ˆæ†ç­‰å‡½æ•¸ï¼‰"""
    
    @staticmethod
    def forward(x):
        """å‰å‘å‚³æ’­ï¼šç›´æ¥è¿”å› x"""
        return x
    
    @staticmethod
    def derivative(x):
        """è¨ˆç®—å°æ•¸ï¼šLinear'(x) = 1"""
        return np.ones_like(x)


# ============================================
# æå¤±å‡½æ•¸
# ============================================

class MSELoss:
    """å‡æ–¹èª¤å·®æå¤±å‡½æ•¸ (Mean Squared Error)"""
    
    @staticmethod
    def get_total_loss(outputs, expects):
        """
        è¨ˆç®—ç¸½æå¤±
        å…¬å¼ï¼šMSE = mean[(é æ¸¬ - çœŸå¯¦)Â²]
        """
        diff = outputs - expects
        squared_diff = diff ** 2
        loss = np.mean(squared_diff)
        return loss
    
    @staticmethod
    def get_output_gradients(outputs, expects):
        """
        è¨ˆç®—æå¤±å°è¼¸å‡ºçš„æ¢¯åº¦
        å…¬å¼ï¼šâˆ‚Loss/âˆ‚O_i = (2/n) * (O_i - E_i)
        """
        n = len(outputs)
        gradients = (2.0 / n) * (outputs - expects)
        return gradients


# ============================================
# ç¥ç¶“ç¶²è·¯
# ============================================

class Network:
    """
    ä¸‰å±¤ç¥ç¶“ç¶²è·¯
    çµæ§‹ï¼šè¼¸å…¥å±¤ -> éš±è—å±¤(ReLU) -> ä¸­é–“å±¤(Linear) -> è¼¸å‡ºå±¤(Linear)
    """
    
    def __init__(self):
        """åˆå§‹åŒ–æ‰€æœ‰æ¬Šé‡"""
        
        # ç¬¬ä¸€å±¤ï¼šè¼¸å…¥å±¤ -> éš±è—å±¤ (2 å€‹ç¥ç¶“å…ƒ)
        self.W1 = np.array([
            [0.5, 0.2],     # X1->H1: 0.5, X2->H1: 0.2
            [0.6, -0.6]     # X1->H2: 0.6, X2->H2: -0.6
        ])
        self.b1 = np.array([0.3, 0.25])
        
        # ç¬¬äºŒå±¤ï¼šéš±è—å±¤ -> ä¸­é–“å±¤ (1 å€‹ç¥ç¶“å…ƒ)
        self.W2 = np.array([0.8, -0.5])
        self.b2 = np.array([0.6])
        
        # ç¬¬ä¸‰å±¤ï¼šä¸­é–“å±¤ -> è¼¸å‡ºå±¤ (2 å€‹ç¥ç¶“å…ƒ)
        self.W3 = np.array([
            [0.6, -0.3]     # M1->O1: 0.6, M1->O2: -0.3
        ])
        self.b3 = np.array([0.4, 0.75])
        
        # æ¢¯åº¦å„²å­˜è®Šæ•¸
        self.grad_W1 = None
        self.grad_b1 = None
        self.grad_W2 = None
        self.grad_b2 = None
        self.grad_W3 = None
        self.grad_b3 = None
        
        # å‰å‘å‚³æ’­ä¸­é–“çµæœ
        self.inputs = None
        self.hidden_linear = None
        self.hidden_output = None
        self.middle_linear = None
        self.middle_output = None
        self.output_linear = None
        self.outputs = None
    
    
    def forward(self, inputs):
        """
        å‰å‘å‚³æ’­
        åƒæ•¸: inputs - è¼¸å…¥å€¼ [X1, X2]
        è¿”å›: outputs - è¼¸å‡ºå€¼ [O1, O2]
        """
        self.inputs = inputs
        
        print("\n=== å‰å‘å‚³æ’­é–‹å§‹ ===")
        print(f"è¼¸å…¥: X1={inputs[0]}, X2={inputs[1]}")
        
        # ç¬¬ä¸€å±¤ï¼šè¼¸å…¥ -> éš±è—å±¤ (ReLU)
        print("\n--- ç¬¬ä¸€å±¤ï¼šè¼¸å…¥ -> éš±è—å±¤ ---")
        self.hidden_linear = np.dot(self.W1.T, inputs) + self.b1
        print(f"éš±è—å±¤ç·šæ€§è¼¸å‡º: {self.hidden_linear}")
        self.hidden_output = ReLU.forward(self.hidden_linear)
        print(f"éš±è—å±¤è¼¸å‡º (ReLU): {self.hidden_output}")
        
        # ç¬¬äºŒå±¤ï¼šéš±è—å±¤ -> ä¸­é–“å±¤ (Linear)
        print("\n--- ç¬¬äºŒå±¤ï¼šéš±è—å±¤ -> ä¸­é–“å±¤ ---")
        self.middle_linear = np.dot(self.W2, self.hidden_output) + self.b2
        print(f"ä¸­é–“å±¤ç·šæ€§è¼¸å‡º: {self.middle_linear}")
        self.middle_output = Linear.forward(self.middle_linear)
        print(f"ä¸­é–“å±¤è¼¸å‡º: {self.middle_output}")
        
        # ç¬¬ä¸‰å±¤ï¼šä¸­é–“å±¤ -> è¼¸å‡ºå±¤ (Linear)
        print("\n--- ç¬¬ä¸‰å±¤ï¼šä¸­é–“å±¤ -> è¼¸å‡ºå±¤ ---")
        self.output_linear = np.dot(self.W3.T, self.middle_output) + self.b3
        print(f"è¼¸å‡ºå±¤ç·šæ€§è¼¸å‡º: {self.output_linear}")
        self.outputs = Linear.forward(self.output_linear)
        print(f"æœ€çµ‚è¼¸å‡º: O1={self.outputs[0]:.4f}, O2={self.outputs[1]:.4f}")
        
        return self.outputs
    
    
    def backward(self, output_gradients):
        """
        åå‘å‚³æ’­
        åƒæ•¸: output_gradients - æå¤±å°è¼¸å‡ºçš„æ¢¯åº¦ âˆ‚Loss/âˆ‚O
        ä½¿ç”¨éˆå¼æ³•å‰‡è¨ˆç®—æ¯å€‹æ¬Šé‡çš„æ¢¯åº¦
        """
        print("\n=== åå‘å‚³æ’­é–‹å§‹ ===")
        print(f"è¼¸å‡ºå±¤çš„æ¢¯åº¦: {output_gradients}")
        
        # ç¬¬ä¸‰å±¤ï¼šè¼¸å‡ºå±¤åå‘å‚³æ’­
        print("\n--- ç¬¬ä¸‰å±¤åå‘ï¼šè¼¸å‡ºå±¤ -> ä¸­é–“å±¤ ---")
        delta_output = output_gradients * Linear.derivative(self.output_linear)
        print(f"è¼¸å‡ºå±¤çš„ delta: {delta_output}")
        
        # è¨ˆç®— W3, b3 çš„æ¢¯åº¦
        self.grad_W3 = np.outer(self.middle_output, delta_output)
        self.grad_b3 = delta_output
        print(f"W3 çš„æ¢¯åº¦:\n{self.grad_W3}")
        print(f"b3 çš„æ¢¯åº¦: {self.grad_b3}")
        
        # å‚³éåˆ°ä¸­é–“å±¤
        delta_middle = np.dot(self.W3, delta_output)
        print(f"å‚³åˆ°ä¸­é–“å±¤çš„æ¢¯åº¦: {delta_middle}")
        
        # ç¬¬äºŒå±¤ï¼šä¸­é–“å±¤åå‘å‚³æ’­
        print("\n--- ç¬¬äºŒå±¤åå‘ï¼šä¸­é–“å±¤ -> éš±è—å±¤ ---")
        delta_middle = delta_middle * Linear.derivative(self.middle_linear)
        print(f"ä¸­é–“å±¤çš„ delta: {delta_middle}")
        
        # è¨ˆç®— W2, b2 çš„æ¢¯åº¦
        self.grad_W2 = delta_middle * self.hidden_output
        self.grad_b2 = delta_middle
        print(f"W2 çš„æ¢¯åº¦: {self.grad_W2}")
        print(f"b2 çš„æ¢¯åº¦: {self.grad_b2}")
        
        # å‚³éåˆ°éš±è—å±¤
        delta_hidden = self.W2 * delta_middle
        print(f"å‚³åˆ°éš±è—å±¤çš„æ¢¯åº¦: {delta_hidden}")
        
        # ç¬¬ä¸€å±¤ï¼šéš±è—å±¤åå‘å‚³æ’­
        print("\n--- ç¬¬ä¸€å±¤åå‘ï¼šéš±è—å±¤ -> è¼¸å…¥å±¤ ---")
        delta_hidden = delta_hidden * ReLU.derivative(self.hidden_linear)
        print(f"éš±è—å±¤çš„ delta (è€ƒæ…® ReLU): {delta_hidden}")
        
        # è¨ˆç®— W1, b1 çš„æ¢¯åº¦
        self.grad_W1 = np.outer(self.inputs, delta_hidden)
        self.grad_b1 = delta_hidden
        print(f"W1 çš„æ¢¯åº¦:\n{self.grad_W1}")
        print(f"b1 çš„æ¢¯åº¦: {self.grad_b1}")
        
        print("\n=== åå‘å‚³æ’­å®Œæˆ ===")
    
    
    def zero_grad(self, learning_rate):
        """
        ä½¿ç”¨æ¢¯åº¦æ›´æ–°æ¬Šé‡
        å…¬å¼ï¼šæ–°æ¬Šé‡ = èˆŠæ¬Šé‡ - (å­¸ç¿’ç‡ Ã— æ¢¯åº¦)
        """
        print(f"\n=== æ›´æ–°æ¬Šé‡ï¼ˆå­¸ç¿’ç‡ = {learning_rate}ï¼‰===")
        
        print("\nèˆŠæ¬Šé‡:")
        print(f"W1:\n{self.W1}")
        print(f"b1: {self.b1}")
        print(f"W2: {self.W2}")
        print(f"b2: {self.b2}")
        print(f"W3:\n{self.W3}")
        print(f"b3: {self.b3}")
        
        # æ›´æ–°æ‰€æœ‰æ¬Šé‡
        self.W1 = self.W1 - learning_rate * self.grad_W1
        self.b1 = self.b1 - learning_rate * self.grad_b1
        self.W2 = self.W2 - learning_rate * self.grad_W2
        self.b2 = self.b2 - learning_rate * self.grad_b2
        self.W3 = self.W3 - learning_rate * self.grad_W3
        self.b3 = self.b3 - learning_rate * self.grad_b3
        
        print("\næ–°æ¬Šé‡:")
        print(f"W1:\n{self.W1}")
        print(f"b1: {self.b1}")
        print(f"W2: {self.W2}")
        print(f"b2: {self.b2}")
        print(f"W3:\n{self.W3}")
        print(f"b3: {self.b3}")
    
    
    def print_weights(self):
        """åˆ—å°æ‰€æœ‰æ¬Šé‡"""
        print("\n" + "="*50)
        print("ç•¶å‰ç¥ç¶“ç¶²è·¯çš„æ‰€æœ‰æ¬Šé‡:")
        print("="*50)
        print(f"\nç¬¬ä¸€å±¤æ¬Šé‡ W1 (è¼¸å…¥->éš±è—):\n{self.W1}")
        print(f"\nç¬¬ä¸€å±¤åå·® b1:\n{self.b1}")
        print(f"\nç¬¬äºŒå±¤æ¬Šé‡ W2 (éš±è—->ä¸­é–“):\n{self.W2}")
        print(f"\nç¬¬äºŒå±¤åå·® b2:\n{self.b2}")
        print(f"\nç¬¬ä¸‰å±¤æ¬Šé‡ W3 (ä¸­é–“->è¼¸å‡º):\n{self.W3}")
        print(f"\nç¬¬ä¸‰å±¤åå·® b3:\n{self.b3}")
        print("="*50)


# ============================================
# Task 1-1: å–®æ¬¡è¨“ç·´
# ============================================

def task_1_1():
    """
    Task 1-1: å–®æ¬¡è¨“ç·´
    åŸ·è¡Œä¸€æ¬¡å®Œæ•´çš„å‰å‘å‚³æ’­ã€åå‘å‚³æ’­å’Œæ¬Šé‡æ›´æ–°
    """
    print("\n" + "="*70)
    print("Task 1-1: å–®æ¬¡è¨“ç·´")
    print("="*70)
    
    # åˆå§‹åŒ–ç¥ç¶“ç¶²è·¯
    nn = Network()
    
    # è¨­å®šè¼¸å…¥å’ŒæœŸæœ›è¼¸å‡º
    inputs = np.array([1.5, 0.5])
    expects = np.array([0.8, 1.0])
    
    # è¨­å®šæå¤±å‡½æ•¸å’Œå­¸ç¿’ç‡
    loss_fn = MSELoss()
    learning_rate = 0.01
    
    print(f"\nè¼¸å…¥: X1={inputs[0]}, X2={inputs[1]}")
    print(f"æœŸæœ›è¼¸å‡º: E1={expects[0]}, E2={expects[1]}")
    print(f"å­¸ç¿’ç‡: {learning_rate}")
    
    # åˆ—å°åˆå§‹æ¬Šé‡
    print("\nåˆå§‹æ¬Šé‡:")
    nn.print_weights()
    
    # å‰å‘å‚³æ’­
    outputs = nn.forward(inputs)
    
    # è¨ˆç®—æå¤±
    loss = loss_fn.get_total_loss(outputs, expects)
    print(f"\nç¸½æå¤±ï¼ˆMSEï¼‰: {loss:.6f}")
    
    # è¨ˆç®—è¼¸å‡ºæ¢¯åº¦
    output_gradients = loss_fn.get_output_gradients(outputs, expects)
    
    # åå‘å‚³æ’­
    nn.backward(output_gradients)
    
    # æ›´æ–°æ¬Šé‡
    nn.zero_grad(learning_rate)
    
    # åˆ—å°æ›´æ–°å¾Œçš„æ¬Šé‡
    print("\næ›´æ–°å¾Œçš„æ¬Šé‡:")
    nn.print_weights()
    
    print("\n" + "="*70)
    print("Task 1-1 å®Œæˆï¼")
    print("="*70)


# ============================================
# Task 1-2: é‡è¤‡è¨“ç·´ 1000 æ¬¡
# ============================================

def task_1_2():
    """
    Task 1-2: é‡è¤‡è¨“ç·´ 1000 æ¬¡
    é€éå¤šæ¬¡è¿­ä»£è¨“ç·´ä¾†æœ€å°åŒ–æå¤±å‡½æ•¸
    """
    print("\n" + "="*70)
    print("Task 1-2: é‡è¤‡è¨“ç·´ 1000 æ¬¡")
    print("="*70)
    
    # åˆå§‹åŒ–ç¥ç¶“ç¶²è·¯
    nn = Network()
    
    # è¨­å®šè¼¸å…¥å’ŒæœŸæœ›è¼¸å‡º
    inputs = np.array([1.5, 0.5])
    expects = np.array([0.8, 1.0])
    
    # è¨­å®šæå¤±å‡½æ•¸å’Œå­¸ç¿’ç‡
    loss_fn = MSELoss()
    learning_rate = 0.01
    
    print(f"\nè¼¸å…¥: X1={inputs[0]}, X2={inputs[1]}")
    print(f"æœŸæœ›è¼¸å‡º: E1={expects[0]}, E2={expects[1]}")
    print(f"å­¸ç¿’ç‡: {learning_rate}")
    print(f"è¨“ç·´æ¬¡æ•¸: 1000 æ¬¡")
    
    print("\né–‹å§‹è¨“ç·´...")
    print("-"*70)
    
    # è¨“ç·´ 1000 æ¬¡
    for i in range(1000):
        # å‰å‘å‚³æ’­
        outputs = nn.forward(inputs)
        
        # è¨ˆç®—æå¤±
        loss = loss_fn.get_total_loss(outputs, expects)
        
        # æ¯ 100 æ¬¡å°ä¸€æ¬¡ï¼Œæˆ–æœ€å¾Œ 10 æ¬¡æ¯æ¬¡éƒ½å°
        if i % 100 == 0 or i >= 990:
            print(f"ç¬¬ {i+1:4d} æ¬¡è¨“ç·´ | æå¤± = {loss:.8f} | "
                  f"è¼¸å‡º O1={outputs[0]:.4f}, O2={outputs[1]:.4f}")
        
        # è¨ˆç®—æ¢¯åº¦
        output_gradients = loss_fn.get_output_gradients(outputs, expects)
        
        # åå‘å‚³æ’­
        nn.backward(output_gradients)
        
        # æ›´æ–°æ¬Šé‡
        nn.zero_grad(learning_rate)
    
    print("-"*70)
    print(f"\nè¨“ç·´å®Œæˆï¼")
    print(f"æœ€çµ‚æå¤±: {loss:.10f}")
    print(f"æœ€çµ‚è¼¸å‡º: O1={outputs[0]:.6f}, O2={outputs[1]:.6f}")
    print(f"æœŸæœ›è¼¸å‡º: E1={expects[0]}, E2={expects[1]}")
    print(f"\næ˜¯å¦æˆåŠŸï¼Ÿ {'âœ“ æ˜¯çš„ï¼æå¤±æ¥è¿‘ 0' if loss < 0.0001 else 'âœ— é‚„éœ€è¦æ›´å¤šè¨“ç·´'}")
    
    # åˆ—å°æœ€çµ‚æ¬Šé‡
    nn.print_weights()
    
    print("\n" + "="*70)
    print("Task 1-2 å®Œæˆï¼")
    print("="*70)


# ============================================
# ä¸»ç¨‹å¼
# ============================================

if __name__ == "__main__":
    print("\n")
    print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    print("â•‘                                                                   â•‘")
    print("â•‘          ç¥ç¶“ç¶²è·¯åå‘å‚³æ’­ - Task 1 è¿´æ­¸ä»»å‹™                        â•‘")
    print("â•‘                                                                   â•‘")
    print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
    
    # åŸ·è¡Œ Task 1-1
    task_1_1()
    
    # æš«åœ
    print("\n\næŒ‰ Enter ç¹¼çºŒåŸ·è¡Œ Task 1-2...")
    input()
    
    # åŸ·è¡Œ Task 1-2
    task_1_2()
    
    print("\n\næ‰€æœ‰ä»»å‹™å®Œæˆï¼ğŸ‰")