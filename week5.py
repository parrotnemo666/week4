"""
Task 1: ç¥ç¶“ç¶²è·¯åå‘å‚³æ’­ - è¿´æ­¸ä»»å‹™
ä½¿ç”¨ MSE æå¤±å‡½æ•¸
"""

import numpy as np

# ============================================
# ç¬¬ä¸€éƒ¨åˆ†ï¼šå®šç¾©æ¿€æ´»å‡½æ•¸åŠå…¶å°æ•¸
# ============================================

class ReLU:
    """
    ReLU æ¿€æ´»å‡½æ•¸
    ç™½è©±ï¼šæŠŠè² æ•¸è®Šæˆ0ï¼Œæ­£æ•¸ä¿æŒä¸è®Š
    """
    @staticmethod
    def forward(x):
        """å‰å‘å‚³æ’­ï¼šè¨ˆç®— ReLU(x)"""
        return np.maximum(0, x)
    
    @staticmethod
    def derivative(x):
        """
        è¨ˆç®—å°æ•¸ï¼šReLU'(x)
        ç™½è©±ï¼šå¦‚æœ x > 0ï¼Œå°æ•¸æ˜¯ 1ï¼›å¦å‰‡æ˜¯ 0
        """
        return (x > 0).astype(float)


class Linear:
    """
    Linear æ¿€æ´»å‡½æ•¸
    ç™½è©±ï¼šä»€éº¼éƒ½ä¸åšï¼Œç›´æ¥è¼¸å‡º
    """
    @staticmethod
    def forward(x):
        """å‰å‘å‚³æ’­ï¼šç›´æ¥è¿”å› x"""
        return x
    
    @staticmethod
    def derivative(x):
        """
        è¨ˆç®—å°æ•¸ï¼šLinear'(x) = 1
        ç™½è©±ï¼šç·šæ€§å‡½æ•¸çš„æ–œç‡æ°¸é æ˜¯ 1
        """
        return np.ones_like(x)


# ============================================
# ç¬¬äºŒéƒ¨åˆ†ï¼šå®šç¾©æå¤±å‡½æ•¸
# ============================================

class MSELoss:
    """
    å‡æ–¹èª¤å·®æå¤±å‡½æ•¸ (Mean Squared Error)
    ç™½è©±ï¼šè¨ˆç®—ã€Œé æ¸¬å€¼ã€å’Œã€ŒçœŸå¯¦å€¼ã€çš„å·®è·
    """
    
    @staticmethod
    def get_total_loss(outputs, expects):
        """
        è¨ˆç®—ç¸½æå¤±
        å…¬å¼ï¼šMSE = å¹³å‡[(é æ¸¬ - çœŸå¯¦)Â²]
        
        ç™½è©±ï¼š
        1. ç®—å‡ºæ¯å€‹è¼¸å‡ºçš„èª¤å·®
        2. èª¤å·®å¹³æ–¹ï¼ˆç¢ºä¿éƒ½æ˜¯æ­£æ•¸ï¼‰
        3. å–å¹³å‡
        """
        # outputs: [O1, O2]
        # expects: [E1, E2]
        diff = outputs - expects  # ç®—èª¤å·®
        squared_diff = diff ** 2   # èª¤å·®å¹³æ–¹
        loss = np.mean(squared_diff)  # å–å¹³å‡
        return loss
    
    @staticmethod
    def get_output_gradients(outputs, expects):
        """
        è¨ˆç®—æå¤±å°è¼¸å‡ºçš„æ¢¯åº¦
        å…¬å¼ï¼šâˆ‚Loss/âˆ‚O_i = (2/n) * (O_i - E_i)
        
        ç™½è©±ï¼š
        - å¦‚æœé æ¸¬å¤ªå¤§ï¼Œæ¢¯åº¦æ˜¯æ­£çš„ï¼ˆè¦æ¸›å°‘ï¼‰
        - å¦‚æœé æ¸¬å¤ªå°ï¼Œæ¢¯åº¦æ˜¯è² çš„ï¼ˆè¦å¢åŠ ï¼‰
        """
        n = len(outputs)  # è¼¸å‡ºçš„æ•¸é‡ï¼ˆé€™è£¡æ˜¯ 2ï¼‰
        gradients = (2.0 / n) * (outputs - expects)
        return gradients


# ============================================
# ç¬¬ä¸‰éƒ¨åˆ†ï¼šå®šç¾©ç¥ç¶“ç¶²è·¯
# ============================================

class Network:
    """
    ä¸‰å±¤ç¥ç¶“ç¶²è·¯
    çµæ§‹ï¼šè¼¸å…¥å±¤ -> éš±è—å±¤(ReLU) -> ä¸­é–“å±¤(Linear) -> è¼¸å‡ºå±¤(Linear)
    """
    
    def __init__(self):
        """
        åˆå§‹åŒ–æ‰€æœ‰æ¬Šé‡
        ç™½è©±ï¼šè¨­å®šç¥ç¶“ç¶²è·¯çš„ã€Œåˆå§‹å…¬å¼ã€
        """
        
        # ===== ç¬¬ä¸€å±¤ï¼šè¼¸å…¥å±¤ -> éš±è—å±¤ =====
        # éš±è—å±¤æœ‰ 2 å€‹ç¥ç¶“å…ƒ
        # å¾åœ–ä¸Šè®€å–åˆå§‹æ¬Šé‡
        
        # è¼¸å…¥åˆ°éš±è—å±¤çš„æ¬Šé‡çŸ©é™£ (2x2)
        # ç¬¬ä¸€è¡Œï¼šX1 å’Œ X2 åˆ°ç¬¬ä¸€å€‹éš±è—ç¥ç¶“å…ƒçš„æ¬Šé‡
        # ç¬¬äºŒè¡Œï¼šX1 å’Œ X2 åˆ°ç¬¬äºŒå€‹éš±è—ç¥ç¶“å…ƒçš„æ¬Šé‡
        self.W1 = np.array([
            [0.5, 0.2],     # X1->H1: 0.5, X2->H1: 0.2
            [0.6, -0.6]     # X1->H2: 0.6, X2->H2: -0.6
        ])
        
        # åå·®åˆ°éš±è—å±¤çš„æ¬Šé‡ (2,)
        self.b1 = np.array([0.3, 0.25])  # Bias->H1: 0.3, Bias->H2: 0.25
        
        
        # ===== ç¬¬äºŒå±¤ï¼šéš±è—å±¤ -> ä¸­é–“å±¤ =====
        # ä¸­é–“å±¤æœ‰ 1 å€‹ç¥ç¶“å…ƒ
        
        # éš±è—å±¤åˆ°ä¸­é–“å±¤çš„æ¬Šé‡ (2,)
        self.W2 = np.array([0.8, -0.5])  # H1->M1: 0.8, H2->M1: -0.5
        
        # åå·®åˆ°ä¸­é–“å±¤çš„æ¬Šé‡ (æ¨™é‡)
        self.b2 = np.array([0.6])  # Bias->M1: 0.6
        
        
        # ===== ç¬¬ä¸‰å±¤ï¼šä¸­é–“å±¤ -> è¼¸å‡ºå±¤ =====
        # è¼¸å‡ºå±¤æœ‰ 2 å€‹ç¥ç¶“å…ƒ
        
        # ä¸­é–“å±¤åˆ°è¼¸å‡ºå±¤çš„æ¬Šé‡çŸ©é™£ (1x2)
        self.W3 = np.array([
            [0.6, -0.3]     # M1->O1: 0.6, M1->O2: -0.3
        ])
        
        # åå·®åˆ°è¼¸å‡ºå±¤çš„æ¬Šé‡ (2,)
        self.b3 = np.array([0.4, 0.75])  # Bias->O1: 0.4, Bias->O2: 0.75
        
        
        # ===== ç”¨ä¾†å­˜æ”¾æ¢¯åº¦çš„è®Šæ•¸ =====
        # åˆå§‹åŒ–ç‚º Noneï¼Œæœƒåœ¨ backward() æ™‚è¨ˆç®—
        self.grad_W1 = None
        self.grad_b1 = None
        self.grad_W2 = None
        self.grad_b2 = None
        self.grad_W3 = None
        self.grad_b3 = None
        
        
        # ===== ç”¨ä¾†å­˜æ”¾å‰å‘å‚³æ’­çš„ä¸­é–“çµæœ =====
        # ç™½è©±ï¼šè¨˜ä½æ¯ä¸€å±¤ç®—å‡ºä¾†çš„å€¼ï¼Œåå‘å‚³æ’­æ™‚æœƒç”¨åˆ°
        self.inputs = None          # è¼¸å…¥å€¼
        self.hidden_linear = None   # éš±è—å±¤ç·šæ€§è¼¸å‡ºï¼ˆReLUä¹‹å‰ï¼‰
        self.hidden_output = None   # éš±è—å±¤è¼¸å‡ºï¼ˆReLUä¹‹å¾Œï¼‰
        self.middle_linear = None   # ä¸­é–“å±¤ç·šæ€§è¼¸å‡º
        self.middle_output = None   # ä¸­é–“å±¤è¼¸å‡º
        self.output_linear = None   # è¼¸å‡ºå±¤ç·šæ€§è¼¸å‡º
        self.outputs = None         # æœ€çµ‚è¼¸å‡º
    
    
    def forward(self, inputs):
        """
        å‰å‘å‚³æ’­
        ç™½è©±ï¼šæ ¹æ“šç¾æœ‰æ¬Šé‡ï¼Œä¸€å±¤ä¸€å±¤è¨ˆç®—åˆ°è¼¸å‡º
        
        åƒæ•¸:
            inputs: è¼¸å…¥å€¼ [X1, X2]
        
        è¿”å›:
            outputs: è¼¸å‡ºå€¼ [O1, O2]
        """
        # è¨˜ä½è¼¸å…¥ï¼ˆåå‘å‚³æ’­æ™‚éœ€è¦ï¼‰
        self.inputs = inputs
        
        print("\n=== å‰å‘å‚³æ’­é–‹å§‹ ===")
        print(f"è¼¸å…¥: X1={inputs[0]}, X2={inputs[1]}")
        
        
        # ===== ç¬¬ä¸€å±¤ï¼šè¨ˆç®—éš±è—å±¤ =====
        print("\n--- ç¬¬ä¸€å±¤ï¼šè¼¸å…¥ -> éš±è—å±¤ ---")
        
        # ç·šæ€§è¨ˆç®—ï¼šZ = W^T * X + b
        # ç™½è©±ï¼šæŠŠè¼¸å…¥ä¹˜ä»¥æ¬Šé‡ï¼Œå†åŠ ä¸Šåå·®
        self.hidden_linear = np.dot(self.W1.T, inputs) + self.b1
        print(f"éš±è—å±¤ç·šæ€§è¼¸å‡ºï¼ˆReLUä¹‹å‰ï¼‰: {self.hidden_linear}")
        
        # æ¿€æ´»å‡½æ•¸ï¼šReLU
        # ç™½è©±ï¼šæŠŠè² æ•¸ç æ‰è®Šæˆ0
        self.hidden_output = ReLU.forward(self.hidden_linear)
        print(f"éš±è—å±¤è¼¸å‡ºï¼ˆReLUä¹‹å¾Œï¼‰: {self.hidden_output}")
        
        
        # ===== ç¬¬äºŒå±¤ï¼šè¨ˆç®—ä¸­é–“å±¤ =====
        print("\n--- ç¬¬äºŒå±¤ï¼šéš±è—å±¤ -> ä¸­é–“å±¤ ---")
        
        # ç·šæ€§è¨ˆç®—
        self.middle_linear = np.dot(self.W2, self.hidden_output) + self.b2
        print(f"ä¸­é–“å±¤ç·šæ€§è¼¸å‡º: {self.middle_linear}")
        
        # æ¿€æ´»å‡½æ•¸ï¼šLinearï¼ˆä¸åšä»»ä½•äº‹ï¼‰
        self.middle_output = Linear.forward(self.middle_linear)
        print(f"ä¸­é–“å±¤è¼¸å‡º: {self.middle_output}")
        
        
        # ===== ç¬¬ä¸‰å±¤ï¼šè¨ˆç®—è¼¸å‡ºå±¤ =====
        print("\n--- ç¬¬ä¸‰å±¤ï¼šä¸­é–“å±¤ -> è¼¸å‡ºå±¤ ---")
        
        # ç·šæ€§è¨ˆç®—
        self.output_linear = np.dot(self.W3.T, self.middle_output) + self.b3
        print(f"è¼¸å‡ºå±¤ç·šæ€§è¼¸å‡º: {self.output_linear}")
        
        # æ¿€æ´»å‡½æ•¸ï¼šLinear
        self.outputs = Linear.forward(self.output_linear)
        print(f"æœ€çµ‚è¼¸å‡º: O1={self.outputs[0]:.4f}, O2={self.outputs[1]:.4f}")
        
        return self.outputs
    
    
    def backward(self, output_gradients):
        """
        åå‘å‚³æ’­
        ç™½è©±ï¼šå¾è¼¸å‡ºå¾€å›æ¨ï¼Œè¨ˆç®—æ¯å€‹æ¬Šé‡çš„æ¢¯åº¦ï¼ˆè©²èª¿æ•´å¤šå°‘ï¼‰
        
        åƒæ•¸:
            output_gradients: æå¤±å°è¼¸å‡ºçš„æ¢¯åº¦ âˆ‚Loss/âˆ‚O
        
        é‡è¦æ¦‚å¿µï¼šéˆå¼æ³•å‰‡ï¼ˆChain Ruleï¼‰
        æ¢¯åº¦çš„è¨ˆç®—å°±åƒã€Œå‚³éè²¬ä»»ã€ï¼Œä¸€å±¤ä¸€å±¤å¾€å›å‚³
        """
        print("\n=== åå‘å‚³æ’­é–‹å§‹ ===")
        print(f"è¼¸å‡ºå±¤çš„æ¢¯åº¦ï¼ˆæå¤±å°è¼¸å‡ºçš„å°æ•¸ï¼‰: {output_gradients}")
        
        
        # ===== ç¬¬ä¸‰å±¤ï¼šè¼¸å‡ºå±¤çš„æ¬Šé‡æ¢¯åº¦ =====
        print("\n--- ç¬¬ä¸‰å±¤åå‘ï¼šè¼¸å‡ºå±¤ -> ä¸­é–“å±¤ ---")
        
        # è¼¸å‡ºå±¤ä½¿ç”¨ Linear æ¿€æ´»ï¼Œå°æ•¸æ˜¯ 1
        # âˆ‚Loss/âˆ‚output_linear = âˆ‚Loss/âˆ‚output Ã— âˆ‚output/âˆ‚output_linear
        #                       = output_gradients Ã— 1
        delta_output = output_gradients * Linear.derivative(self.output_linear)
        print(f"è¼¸å‡ºå±¤çš„ delta: {delta_output}")
        
        # è¨ˆç®— W3 çš„æ¢¯åº¦
        # âˆ‚Loss/âˆ‚W3 = delta_output Ã— middle_output
        # ç™½è©±ï¼šé€™å€‹æ¬Šé‡çš„æ¢¯åº¦ = é€™å±¤çš„èª¤å·®ä¿¡è™Ÿ Ã— ä¸Šä¸€å±¤çš„è¼¸å‡º
        self.grad_W3 = np.outer(self.middle_output, delta_output)
        print(f"W3 çš„æ¢¯åº¦:\n{self.grad_W3}")
        
        # è¨ˆç®— b3 çš„æ¢¯åº¦
        # åå·®çš„æ¢¯åº¦å°±æ˜¯é€™å±¤çš„ deltaï¼ˆå› ç‚ºåå·®çš„è¼¸å…¥æ°¸é æ˜¯1ï¼‰
        self.grad_b3 = delta_output
        print(f"b3 çš„æ¢¯åº¦: {self.grad_b3}")
        
        # æŠŠèª¤å·®ä¿¡è™Ÿå‚³åˆ°ä¸­é–“å±¤
        # âˆ‚Loss/âˆ‚middle_output = W3 Ã— delta_output
        # ç™½è©±ï¼šæ ¹æ“šæ¬Šé‡çš„å¤§å°ï¼ŒæŠŠèª¤å·®ã€Œåˆ†é…ã€åˆ°ä¸Šä¸€å±¤
        delta_middle = np.dot(self.W3, delta_output)
        print(f"å‚³åˆ°ä¸­é–“å±¤çš„æ¢¯åº¦: {delta_middle}")
        
        
        # ===== ç¬¬äºŒå±¤ï¼šä¸­é–“å±¤çš„æ¬Šé‡æ¢¯åº¦ =====
        print("\n--- ç¬¬äºŒå±¤åå‘ï¼šä¸­é–“å±¤ -> éš±è—å±¤ ---")
        
        # ä¸­é–“å±¤ä½¿ç”¨ Linear æ¿€æ´»ï¼Œå°æ•¸æ˜¯ 1
        delta_middle = delta_middle * Linear.derivative(self.middle_linear)
        print(f"ä¸­é–“å±¤çš„ delta: {delta_middle}")
        
        # è¨ˆç®— W2 çš„æ¢¯åº¦
        self.grad_W2 = delta_middle * self.hidden_output
        print(f"W2 çš„æ¢¯åº¦: {self.grad_W2}")
        
        # è¨ˆç®— b2 çš„æ¢¯åº¦
        self.grad_b2 = delta_middle
        print(f"b2 çš„æ¢¯åº¦: {self.grad_b2}")
        
        # æŠŠèª¤å·®ä¿¡è™Ÿå‚³åˆ°éš±è—å±¤
        delta_hidden = self.W2 * delta_middle
        print(f"å‚³åˆ°éš±è—å±¤çš„æ¢¯åº¦: {delta_hidden}")
        
        
        # ===== ç¬¬ä¸€å±¤ï¼šéš±è—å±¤çš„æ¬Šé‡æ¢¯åº¦ =====
        print("\n--- ç¬¬ä¸€å±¤åå‘ï¼šéš±è—å±¤ -> è¼¸å…¥å±¤ ---")
        
        # éš±è—å±¤ä½¿ç”¨ ReLU æ¿€æ´»
        # é‡è¦ï¼šå¦‚æœæŸå€‹ç¥ç¶“å…ƒåœ¨å‰å‘å‚³æ’­æ™‚è¼¸å‡ºæ˜¯ 0ï¼ˆæ²’è¢«æ¿€æ´»ï¼‰ï¼Œ
        #       é‚£å®ƒçš„æ¢¯åº¦ä¹Ÿæ˜¯ 0ï¼ˆæ²’è²¬ä»»ï¼‰
        delta_hidden = delta_hidden * ReLU.derivative(self.hidden_linear)
        print(f"éš±è—å±¤çš„ deltaï¼ˆè€ƒæ…®ReLUï¼‰: {delta_hidden}")
        
        # è¨ˆç®— W1 çš„æ¢¯åº¦
        # W1 æ˜¯ 2x2 çŸ©é™£ï¼Œè¦å°æ¯å€‹å…ƒç´ è¨ˆç®—æ¢¯åº¦
        self.grad_W1 = np.outer(self.inputs, delta_hidden)
        print(f"W1 çš„æ¢¯åº¦:\n{self.grad_W1}")
        
        # è¨ˆç®— b1 çš„æ¢¯åº¦
        self.grad_b1 = delta_hidden
        print(f"b1 çš„æ¢¯åº¦: {self.grad_b1}")
        
        print("\n=== åå‘å‚³æ’­å®Œæˆ ===")
    
    
    def zero_grad(self, learning_rate):
        """
        ä½¿ç”¨æ¢¯åº¦æ›´æ–°æ¬Šé‡
        ç™½è©±ï¼šæ ¹æ“šè¨ˆç®—å‡ºçš„æ¢¯åº¦ï¼Œèª¿æ•´æ¯å€‹æ¬Šé‡
        
        å…¬å¼ï¼šæ–°æ¬Šé‡ = èˆŠæ¬Šé‡ - (å­¸ç¿’ç‡ Ã— æ¢¯åº¦)
        
        ç‚ºä»€éº¼æ˜¯ã€Œæ¸›å»ã€ï¼Ÿ
        - æ¢¯åº¦æŒ‡å‘ã€Œæå¤±å¢åŠ ã€çš„æ–¹å‘
        - æˆ‘å€‘è¦æ¸›å°‘æå¤±ï¼Œæ‰€ä»¥è¦å¾€ç›¸åæ–¹å‘èµ°
        - å­¸ç¿’ç‡æ§åˆ¶ã€Œæ­¥ä¼å¤§å°ã€
        """
        print(f"\n=== æ›´æ–°æ¬Šé‡ï¼ˆå­¸ç¿’ç‡ = {learning_rate}ï¼‰===")
        
        print("\nèˆŠæ¬Šé‡:")
        print(f"W1:\n{self.W1}")
        print(f"b1: {self.b1}")
        print(f"W2: {self.W2}")
        print(f"b2: {self.b2}")
        print(f"W3:\n{self.W3}")
        print(f"b3: {self.b3}")
        
        # æ›´æ–°æ‰€æœ‰æ¬Šé‡
        self.W1 = self.W1 - learning_rate * self.grad_W1
        self.b1 = self.b1 - learning_rate * self.grad_b1
        self.W2 = self.W2 - learning_rate * self.grad_W2
        self.b2 = self.b2 - learning_rate * self.grad_b2
        self.W3 = self.W3 - learning_rate * self.grad_W3
        self.b3 = self.b3 - learning_rate * self.grad_b3
        
        print("\næ–°æ¬Šé‡:")
        print(f"W1:\n{self.W1}")
        print(f"b1: {self.b1}")
        print(f"W2: {self.W2}")
        print(f"b2: {self.b2}")
        print(f"W3:\n{self.W3}")
        print(f"b3: {self.b3}")
    
    
    def print_weights(self):
        """åˆ—å°æ‰€æœ‰æ¬Šé‡ï¼ˆæ–¹ä¾¿æª¢æŸ¥ï¼‰"""
        print("\n" + "="*50)
        print("ç•¶å‰ç¥ç¶“ç¶²è·¯çš„æ‰€æœ‰æ¬Šé‡:")
        print("="*50)
        print(f"\nç¬¬ä¸€å±¤æ¬Šé‡ W1 (è¼¸å…¥->éš±è—):\n{self.W1}")
        print(f"\nç¬¬ä¸€å±¤åå·® b1:\n{self.b1}")
        print(f"\nç¬¬äºŒå±¤æ¬Šé‡ W2 (éš±è—->ä¸­é–“):\n{self.W2}")
        print(f"\nç¬¬äºŒå±¤åå·® b2:\n{self.b2}")
        print(f"\nç¬¬ä¸‰å±¤æ¬Šé‡ W3 (ä¸­é–“->è¼¸å‡º):\n{self.W3}")
        print(f"\nç¬¬ä¸‰å±¤åå·® b3:\n{self.b3}")
        print("="*50)


# ============================================
# ç¬¬å››éƒ¨åˆ†ï¼šTask 1-1 å¯¦ç¾
# ============================================

def task_1_1():
    """
    Task 1-1: å–®æ¬¡è¨“ç·´
    ç™½è©±ï¼š
    1. å»ºç«‹ç¥ç¶“ç¶²è·¯
    2. ç”¨åˆå§‹æ¬Šé‡ç®—ä¸€æ¬¡
    3. çœ‹çœ‹éŒ¯å¤šå°‘
    4. ç”¨åå‘å‚³æ’­ç®—å‡ºè©²æ€éº¼èª¿æ•´
    5. èª¿æ•´ä¸€æ¬¡æ¬Šé‡
    """
    print("\n" + "="*70)
    print("Task 1-1: å–®æ¬¡è¨“ç·´")
    print("="*70)
    
    # åˆå§‹åŒ–ç¥ç¶“ç¶²è·¯
    nn = Network()
    
    # è¨­å®šè¼¸å…¥å’ŒæœŸæœ›è¼¸å‡º
    inputs = np.array([1.5, 0.5])
    expects = np.array([0.8, 1.0])
    
    # è¨­å®šæå¤±å‡½æ•¸å’Œå­¸ç¿’ç‡
    loss_fn = MSELoss()
    learning_rate = 0.01
    
    print(f"\nè¼¸å…¥: X1={inputs[0]}, X2={inputs[1]}")
    print(f"æœŸæœ›è¼¸å‡º: E1={expects[0]}, E2={expects[1]}")
    print(f"å­¸ç¿’ç‡: {learning_rate}")
    
    # åˆ—å°åˆå§‹æ¬Šé‡
    print("\nåˆå§‹æ¬Šé‡:")
    nn.print_weights()
    
    # å‰å‘å‚³æ’­
    outputs = nn.forward(inputs)
    
    # è¨ˆç®—æå¤±
    loss = loss_fn.get_total_loss(outputs, expects)
    print(f"\nç¸½æå¤±ï¼ˆMSEï¼‰: {loss:.6f}")
    print(f"ç™½è©±ï¼šé æ¸¬å€¼å’ŒçœŸå¯¦å€¼çš„å¹³å‡èª¤å·®å¹³æ–¹æ˜¯ {loss:.6f}")
    
    # è¨ˆç®—è¼¸å‡ºæ¢¯åº¦
    output_gradients = loss_fn.get_output_gradients(outputs, expects)
    
    # åå‘å‚³æ’­
    nn.backward(output_gradients)
    
    # æ›´æ–°æ¬Šé‡
    nn.zero_grad(learning_rate)
    
    # åˆ—å°æ›´æ–°å¾Œçš„æ¬Šé‡
    print("\næ›´æ–°å¾Œçš„æ¬Šé‡:")
    nn.print_weights()
    
    print("\n" + "="*70)
    print("Task 1-1 å®Œæˆï¼")
    print("="*70)


# ============================================
# ç¬¬äº”éƒ¨åˆ†ï¼šTask 1-2 å¯¦ç¾
# ============================================

def task_1_2():
    """
    Task 1-2: é‡è¤‡è¨“ç·´ 1000 æ¬¡
    ç™½è©±ï¼š
    - é‡è¤‡åš 1000 æ¬¡ã€Œç®—ç­”æ¡ˆ -> çœ‹éŒ¯å¤šå°‘ -> èª¿æ•´æ¬Šé‡ã€
    - æ¯æ¬¡éƒ½æœƒè¶Šä¾†è¶Šæº–ç¢º
    - æœ€å¾Œæå¤±æ‡‰è©²æ¥è¿‘ 0
    """
    print("\n" + "="*70)
    print("Task 1-2: é‡è¤‡è¨“ç·´ 1000 æ¬¡")
    print("="*70)
    
    # åˆå§‹åŒ–ç¥ç¶“ç¶²è·¯
    nn = Network()
    
    # è¨­å®šè¼¸å…¥å’ŒæœŸæœ›è¼¸å‡º
    inputs = np.array([1.5, 0.5])
    expects = np.array([0.8, 1.0])
    
    # è¨­å®šæå¤±å‡½æ•¸å’Œå­¸ç¿’ç‡
    loss_fn = MSELoss()
    learning_rate = 0.01
    
    print(f"\nè¼¸å…¥: X1={inputs[0]}, X2={inputs[1]}")
    print(f"æœŸæœ›è¼¸å‡º: E1={expects[0]}, E2={expects[1]}")
    print(f"å­¸ç¿’ç‡: {learning_rate}")
    print(f"è¨“ç·´æ¬¡æ•¸: 1000 æ¬¡")
    
    print("\né–‹å§‹è¨“ç·´...")
    print("-"*70)
    
    # è¨“ç·´ 1000 æ¬¡
    for i in range(1000):
        # å‰å‘å‚³æ’­ï¼ˆä¸å°å‡ºè©³ç´°éç¨‹ï¼‰
        outputs = nn.forward(inputs)
        
        # è¨ˆç®—æå¤±
        loss = loss_fn.get_total_loss(outputs, expects)
        
        # æ¯ 100 æ¬¡å°ä¸€æ¬¡ï¼Œæˆ–æ˜¯æœ€å¾Œ 10 æ¬¡æ¯æ¬¡éƒ½å°
        if i % 100 == 0 or i >= 990:
            print(f"ç¬¬ {i+1:4d} æ¬¡è¨“ç·´ | æå¤± = {loss:.8f} | "
                  f"è¼¸å‡º O1={outputs[0]:.4f}, O2={outputs[1]:.4f}")
        
        # è¨ˆç®—æ¢¯åº¦
        output_gradients = loss_fn.get_output_gradients(outputs, expects)
        
        # åå‘å‚³æ’­ï¼ˆä¸å°å‡ºè©³ç´°éç¨‹ï¼‰
        nn.backward(output_gradients)
        
        # æ›´æ–°æ¬Šé‡ï¼ˆä¸å°å‡ºè©³ç´°éç¨‹ï¼‰
        nn.zero_grad(learning_rate)
    
    print("-"*70)
    print(f"\nè¨“ç·´å®Œæˆï¼")
    print(f"æœ€çµ‚æå¤±: {loss:.10f}")
    print(f"æœ€çµ‚è¼¸å‡º: O1={outputs[0]:.6f}, O2={outputs[1]:.6f}")
    print(f"æœŸæœ›è¼¸å‡º: E1={expects[0]}, E2={expects[1]}")
    print(f"\næ˜¯å¦æˆåŠŸï¼Ÿ {'âœ“ æ˜¯çš„ï¼æå¤±æ¥è¿‘ 0' if loss < 0.0001 else 'âœ— é‚„éœ€è¦æ›´å¤šè¨“ç·´'}")
    
    # åˆ—å°æœ€çµ‚æ¬Šé‡
    nn.print_weights()
    
    print("\n" + "="*70)
    print("Task 1-2 å®Œæˆï¼")
    print("="*70)


# ============================================
# ä¸»ç¨‹å¼
# ============================================

if __name__ == "__main__":
    print("\n")
    print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    print("â•‘                                                                   â•‘")
    print("â•‘          ç¥ç¶“ç¶²è·¯åå‘å‚³æ’­ - Task 1 è¿´æ­¸ä»»å‹™                        â•‘")
    print("â•‘                                                                   â•‘")
    print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
    
    # åŸ·è¡Œ Task 1-1
    task_1_1()
    
    # æš«åœä¸€ä¸‹
    print("\n\næŒ‰ Enter ç¹¼çºŒåŸ·è¡Œ Task 1-2...")
    input()
    
    # åŸ·è¡Œ Task 1-2
    task_1_2()
    
    print("\n\næ‰€æœ‰ä»»å‹™å®Œæˆï¼ğŸ‰")
    