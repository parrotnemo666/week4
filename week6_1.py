"""
Task 1: é æ¸¬é«”é‡çš„å›æ­¸ç¥ç¶“ç¶²çµ¡
ä½¿ç”¨æ€§åˆ¥å’Œèº«é«˜é æ¸¬é«”é‡

ä½œè€…ï¼šæ ¹æ“š WeHelp Week 5-6 èª²ç¨‹å…§å®¹å¯¦ä½œ

å¢å¼·ç‰ˆåŠŸèƒ½ï¼š
- è¨˜éŒ„æ¯å€‹ epoch çš„æå¤±å’Œèª¤å·®
- è‡ªå‹•ä¿å­˜è¨“ç·´æ­·å²åˆ° CSV æª”æ¡ˆ
- åˆ†æè¨“ç·´éç¨‹å’Œæ”¶æ–‚æƒ…æ³
"""

import csv
import random
import math
import time

# ============================================================
# ç¬¬ä¸€æ­¥ï¼šå»ºç«‹åŸºç¤ç¥ç¶“å…ƒé¡åˆ¥
# ============================================================

class Neuron:
    """
    å–®ä¸€ç¥ç¶“å…ƒ
    
    åŠŸèƒ½ï¼š
    1. å„²å­˜æ¬Šé‡å’Œåå·®
    2. è¨ˆç®—åŠ æ¬Šå’Œï¼ˆweighted sumï¼‰
    3. æ‡‰ç”¨æ¿€æ´»å‡½æ•¸
    4. å„²å­˜æ¢¯åº¦ç”¨æ–¼åå‘å‚³æ’­
    """
    
    def __init__(self, num_inputs, activation='relu'):
        """
        åˆå§‹åŒ–ç¥ç¶“å…ƒ
        
        åƒæ•¸ï¼š
        - num_inputs: è¼¸å…¥çš„æ•¸é‡
        - activation: æ¿€æ´»å‡½æ•¸é¡å‹ ('relu', 'sigmoid', 'linear')
        """
        # ä½¿ç”¨å°çš„éš¨æ©Ÿæ•¸åˆå§‹åŒ–æ¬Šé‡ï¼ˆé¿å…å°ç¨±æ€§å•é¡Œï¼‰
        self.weights = [random.uniform(-0.5, 0.5) for _ in range(num_inputs)]
        self.bias = random.uniform(-0.5, 0.5)
        self.activation = activation
        
        # ç”¨æ–¼å„²å­˜å‰å‘å‚³æ’­çš„ä¸­é–“å€¼ï¼ˆåå‘å‚³æ’­æœƒç”¨åˆ°ï¼‰
        self.inputs = None
        self.weighted_sum = None
        self.output = None
        
        # ç”¨æ–¼å„²å­˜æ¢¯åº¦
        self.weight_gradients = [0] * num_inputs
        self.bias_gradient = 0
    
    def forward(self, inputs):
        """
        å‰å‘å‚³æ’­
        
        æ­¥é©Ÿï¼š
        1. è¨ˆç®—åŠ æ¬Šå’Œï¼šz = w1*x1 + w2*x2 + ... + b
        2. æ‡‰ç”¨æ¿€æ´»å‡½æ•¸ï¼šoutput = activation(z)
        """
        self.inputs = inputs
        
        # è¨ˆç®—åŠ æ¬Šå’Œ
        self.weighted_sum = sum(w * x for w, x in zip(self.weights, inputs)) + self.bias
        
        # æ‡‰ç”¨æ¿€æ´»å‡½æ•¸
        if self.activation == 'relu':
            self.output = max(0, self.weighted_sum)
        elif self.activation == 'sigmoid':
            self.output = 1 / (1 + math.exp(-self.weighted_sum))
        elif self.activation == 'linear':
            self.output = self.weighted_sum
        else:
            raise ValueError(f"Unknown activation: {self.activation}")
        
        return self.output
    
    def backward(self, upstream_gradient):
        """
        åå‘å‚³æ’­
        
        åƒæ•¸ï¼š
        - upstream_gradient: å¾å¾Œé¢å±¤å‚³å›ä¾†çš„æ¢¯åº¦
        
        è¿”å›ï¼š
        - å‚³çµ¦å‰ä¸€å±¤çš„æ¢¯åº¦
        """
        # è¨ˆç®—æ¿€æ´»å‡½æ•¸çš„å°æ•¸
        if self.activation == 'relu':
            activation_derivative = 1 if self.weighted_sum > 0 else 0
        elif self.activation == 'sigmoid':
            activation_derivative = self.output * (1 - self.output)
        elif self.activation == 'linear':
            activation_derivative = 1
        
        # è¨ˆç®—å°åŠ æ¬Šå’Œçš„æ¢¯åº¦
        delta = upstream_gradient * activation_derivative
        
        # è¨ˆç®—å°æ¬Šé‡å’Œåå·®çš„æ¢¯åº¦
        self.weight_gradients = [delta * x for x in self.inputs]
        self.bias_gradient = delta
        
        # è¨ˆç®—å‚³çµ¦å‰ä¸€å±¤çš„æ¢¯åº¦
        input_gradients = [delta * w for w in self.weights]
        
        return input_gradients


class Layer:
    """
    ç¥ç¶“ç¶²çµ¡å±¤ï¼ˆåŒ…å«å¤šå€‹ç¥ç¶“å…ƒï¼‰
    """
    
    def __init__(self, num_neurons, num_inputs_per_neuron, activation='relu'):
        """
        åˆå§‹åŒ–å±¤
        
        åƒæ•¸ï¼š
        - num_neurons: é€™ä¸€å±¤æœ‰å¹¾å€‹ç¥ç¶“å…ƒ
        - num_inputs_per_neuron: æ¯å€‹ç¥ç¶“å…ƒæ¥æ”¶å¹¾å€‹è¼¸å…¥
        - activation: æ¿€æ´»å‡½æ•¸é¡å‹
        """
        self.neurons = [
            Neuron(num_inputs_per_neuron, activation) 
            for _ in range(num_neurons)
        ]
    
    def forward(self, inputs):
        """å‰å‘å‚³æ’­ï¼šæ¯å€‹ç¥ç¶“å…ƒéƒ½è¨ˆç®—è¼¸å‡º"""
        return [neuron.forward(inputs) for neuron in self.neurons]
    
    def backward(self, upstream_gradients):
        """
        åå‘å‚³æ’­
        
        åƒæ•¸ï¼š
        - upstream_gradients: æ¯å€‹ç¥ç¶“å…ƒå°æ‡‰çš„æ¢¯åº¦åˆ—è¡¨
        
        è¿”å›ï¼š
        - å°è¼¸å…¥çš„æ¢¯åº¦
        """
        # æ¯å€‹ç¥ç¶“å…ƒè¨ˆç®—è‡ªå·±çš„æ¢¯åº¦
        input_gradients_list = [
            neuron.backward(grad) 
            for neuron, grad in zip(self.neurons, upstream_gradients)
        ]
        
        # å°‡æ‰€æœ‰ç¥ç¶“å…ƒå°ç›¸åŒè¼¸å…¥çš„æ¢¯åº¦åŠ ç¸½
        num_inputs = len(input_gradients_list[0])
        input_gradients = [
            sum(grads[i] for grads in input_gradients_list)
            for i in range(num_inputs)
        ]
        
        return input_gradients
    
    def update_weights(self, learning_rate):
        """æ›´æ–°æ‰€æœ‰ç¥ç¶“å…ƒçš„æ¬Šé‡å’Œåå·®"""
        for neuron in self.neurons:
            # w_new = w_old - learning_rate * gradient
            neuron.weights = [
                w - learning_rate * grad 
                for w, grad in zip(neuron.weights, neuron.weight_gradients)
            ]
            neuron.bias -= learning_rate * neuron.bias_gradient
    
    def zero_grad(self):
        """æ¸…ç©ºæ¢¯åº¦ï¼ˆç‚ºä¸‹ä¸€æ¬¡è¨“ç·´æº–å‚™ï¼‰"""
        for neuron in self.neurons:
            neuron.weight_gradients = [0] * len(neuron.weight_gradients)
            neuron.bias_gradient = 0


class Network:
    """
    å®Œæ•´çš„ç¥ç¶“ç¶²çµ¡ï¼ˆå¤šå±¤å †ç–Šï¼‰
    """
    
    def __init__(self, layer_configs):
        """
        åˆå§‹åŒ–ç¶²çµ¡
        
        åƒæ•¸ï¼š
        - layer_configs: åˆ—è¡¨ï¼Œæ¯å€‹å…ƒç´ æ˜¯ (num_neurons, activation)
          ä¾‹å¦‚ï¼š[(4, 'relu'), (1, 'linear')] è¡¨ç¤ºä¸€å€‹éš±è—å±¤ 4 å€‹ç¥ç¶“å…ƒç”¨ ReLUï¼Œ
                è¼¸å‡ºå±¤ 1 å€‹ç¥ç¶“å…ƒç”¨ Linear
        """
        self.layers = []
        
        # å»ºç«‹æ¯ä¸€å±¤
        for i, (num_neurons, activation) in enumerate(layer_configs):
            if i == 0:
                # ç¬¬ä¸€å±¤éœ€è¦çŸ¥é“è¼¸å…¥ç¶­åº¦ï¼Œé€™è£¡å…ˆä¸å»ºç«‹ï¼Œç­‰ forward æ™‚å†è™•ç†
                self.first_layer_config = (num_neurons, activation)
            else:
                # å¾ŒçºŒå±¤çš„è¼¸å…¥ç¶­åº¦ = å‰ä¸€å±¤çš„ç¥ç¶“å…ƒæ•¸é‡
                num_inputs = layer_configs[i-1][0]
                layer = Layer(num_neurons, num_inputs, activation)
                self.layers.append(layer)
        
        self.first_layer_built = False
    
    def forward(self, inputs):
        """
        å‰å‘å‚³æ’­
        
        åƒæ•¸ï¼š
        - inputs: è¼¸å…¥åˆ—è¡¨
        
        è¿”å›ï¼š
        - è¼¸å‡ºåˆ—è¡¨
        """
        # å¦‚æœæ˜¯ç¬¬ä¸€æ¬¡ forwardï¼Œå»ºç«‹ç¬¬ä¸€å±¤
        if not self.first_layer_built:
            num_neurons, activation = self.first_layer_config
            first_layer = Layer(num_neurons, len(inputs), activation)
            self.layers.insert(0, first_layer)
            self.first_layer_built = True
        
        # ä¾åºé€šéæ¯ä¸€å±¤
        outputs = inputs
        for layer in self.layers:
            outputs = layer.forward(outputs)
        
        return outputs
    
    def backward(self, output_gradients):
        """
        åå‘å‚³æ’­
        
        åƒæ•¸ï¼š
        - output_gradients: å°è¼¸å‡ºçš„æ¢¯åº¦
        """
        gradients = output_gradients
        for layer in reversed(self.layers):
            gradients = layer.backward(gradients)
    
    def zero_grad(self, learning_rate):
        """
        æ›´æ–°æ¬Šé‡ä¸¦æ¸…ç©ºæ¢¯åº¦
        
        æ³¨æ„ï¼šé€™è£¡çš„æ–¹æ³•åç¨±å« zero_gradï¼Œä½†å¯¦éš›ä¸ŠåŒ…å«äº†æ¬Šé‡æ›´æ–°
        ï¼ˆæ ¹æ“šä½œæ¥­ç¯„ä¾‹çš„ç¨‹å¼ç¢¼çµæ§‹ï¼‰
        """
        for layer in self.layers:
            layer.update_weights(learning_rate)
            layer.zero_grad()


# ============================================================
# ç¬¬äºŒæ­¥ï¼šå»ºç«‹æå¤±å‡½æ•¸é¡åˆ¥
# ============================================================

class MSELoss:
    """
    å‡æ–¹èª¤å·®æå¤±å‡½æ•¸ï¼ˆMean Squared Errorï¼‰
    
    ç”¨æ–¼å›æ­¸ä»»å‹™
    å…¬å¼ï¼šLoss = (output - expected)^2
    """
    
    def get_total_loss(self, outputs, expected):
        """
        è¨ˆç®—æå¤±å€¼
        
        åƒæ•¸ï¼š
        - outputs: ç¶²çµ¡è¼¸å‡ºï¼ˆåˆ—è¡¨ï¼‰
        - expected: æœŸæœ›å€¼ï¼ˆå–®ä¸€æ•¸å€¼ï¼‰
        
        è¿”å›ï¼š
        - æå¤±å€¼
        """
        # å› ç‚ºè¼¸å‡ºå±¤åªæœ‰ä¸€å€‹ç¥ç¶“å…ƒï¼Œæ‰€ä»¥ outputs[0] å°±æ˜¯é æ¸¬çš„é«”é‡
        prediction = outputs[0]
        return (prediction - expected) ** 2
    
    def get_output_gradients(self, outputs, expected):
        """
        è¨ˆç®—æå¤±å°è¼¸å‡ºçš„æ¢¯åº¦
        
        å…¬å¼ï¼šd(Loss)/d(output) = 2 * (output - expected)
        
        è¿”å›ï¼š
        - æ¢¯åº¦åˆ—è¡¨
        """
        prediction = outputs[0]
        gradient = 2 * (prediction - expected)
        return [gradient]


# ============================================================
# ç¬¬ä¸‰æ­¥ï¼šè³‡æ–™é è™•ç†
# ============================================================

def load_and_preprocess_data(filename):
    """
    è¼‰å…¥ä¸¦é è™•ç†è³‡æ–™
    
    æ­¥é©Ÿï¼š
    1. è®€å– CSV
    2. æ€§åˆ¥ç·¨ç¢¼ï¼šMale=0, Female=1
    3. æ¨™æº–åŒ–èº«é«˜å’Œé«”é‡
    
    è¿”å›ï¼š
    - xs: è¼¸å…¥åˆ—è¡¨ï¼ˆæ¯å€‹å…ƒç´ æ˜¯ [æ€§åˆ¥ç·¨ç¢¼, æ¨™æº–åŒ–èº«é«˜]ï¼‰
    - es: æœŸæœ›è¼¸å‡ºåˆ—è¡¨ï¼ˆæ¨™æº–åŒ–é«”é‡ï¼‰
    - height_mean, height_std: èº«é«˜çš„å¹³å‡å€¼å’Œæ¨™æº–å·®ï¼ˆç”¨æ–¼é‚„åŸï¼‰
    - weight_mean, weight_std: é«”é‡çš„å¹³å‡å€¼å’Œæ¨™æº–å·®ï¼ˆç”¨æ–¼é‚„åŸï¼‰
    """
    print("\n" + "="*60)
    print("ğŸ“ è¼‰å…¥ä¸¦é è™•ç†è³‡æ–™")
    print("="*60)
    
    # è®€å–è³‡æ–™
    data = []
    with open(filename, 'r') as f:
        reader = csv.DictReader(f)
        for row in reader:
            data.append({
                'Gender': row['Gender'],
                'Height': float(row['Height']),
                'Weight': float(row['Weight'])
            })
    
    print(f"âœ“ è¼‰å…¥ {len(data)} ç­†è³‡æ–™")
    
    # æ­¥é©Ÿ 1ï¼šæ€§åˆ¥ç·¨ç¢¼
    print("\næ­¥é©Ÿ 1ï¼šæ€§åˆ¥ç·¨ç¢¼")
    print("  Male â†’ 0")
    print("  Female â†’ 1")
    
    for d in data:
        d['Gender_Encoded'] = 0 if d['Gender'] == 'Male' else 1
    
    # æ­¥é©Ÿ 2ï¼šè¨ˆç®—çµ±è¨ˆå€¼ï¼ˆç”¨æ–¼æ¨™æº–åŒ–ï¼‰
    heights = [d['Height'] for d in data]
    weights = [d['Weight'] for d in data]
    
    height_mean = sum(heights) / len(heights)
    height_std = (sum((h - height_mean)**2 for h in heights) / len(heights)) ** 0.5
    
    weight_mean = sum(weights) / len(weights)
    weight_std = (sum((w - weight_mean)**2 for w in weights) / len(weights)) ** 0.5
    
    print("\næ­¥é©Ÿ 2ï¼šè¨ˆç®—çµ±è¨ˆå€¼")
    print(f"  èº«é«˜å¹³å‡å€¼: {height_mean:.2f} è‹±å‹")
    print(f"  èº«é«˜æ¨™æº–å·®: {height_std:.2f} è‹±å‹")
    print(f"  é«”é‡å¹³å‡å€¼: {weight_mean:.2f} ç£…")
    print(f"  é«”é‡æ¨™æº–å·®: {weight_std:.2f} ç£…")
    
    # æ­¥é©Ÿ 3ï¼šæ¨™æº–åŒ–
    print("\næ­¥é©Ÿ 3ï¼šæ¨™æº–åŒ–")
    print("  å…¬å¼ï¼šnormalized = (value - mean) / std")
    
    for d in data:
        d['Height_Normalized'] = (d['Height'] - height_mean) / height_std
        d['Weight_Normalized'] = (d['Weight'] - weight_mean) / weight_std
    
    # æ‰“å°å‰ 3 ç­†ç¯„ä¾‹
    print("\næ¨™æº–åŒ–å‰ vs æ¨™æº–åŒ–å¾Œï¼ˆå‰ 3 ç­†ï¼‰:")
    for i in range(3):
        d = data[i]
        print(f"  åŸå§‹: æ€§åˆ¥={d['Gender']}, èº«é«˜={d['Height']:.2f}, é«”é‡={d['Weight']:.2f}")
        print(f"  æ¨™æº–åŒ–: æ€§åˆ¥={d['Gender_Encoded']}, èº«é«˜={d['Height_Normalized']:.4f}, é«”é‡={d['Weight_Normalized']:.4f}")
        print()
    
    # æº–å‚™è¨“ç·´è³‡æ–™
    xs = [[d['Gender_Encoded'], d['Height_Normalized']] for d in data]
    es = [d['Weight_Normalized'] for d in data]
    
    return xs, es, height_mean, height_std, weight_mean, weight_std


# ============================================================
# ç¬¬å››æ­¥ï¼šè¨“ç·´æµç¨‹
# ============================================================

def train_model(xs, es, epochs=500, learning_rate=0.01, print_every=50, 
                weight_mean=None, weight_std=None, save_history=True):
    """
    è¨“ç·´ç¥ç¶“ç¶²çµ¡ï¼ˆå¢å¼·ç‰ˆ - è¨˜éŒ„æ‰€æœ‰è¨“ç·´éç¨‹ï¼‰
    
    åƒæ•¸ï¼š
    - xs: è¼¸å…¥è³‡æ–™
    - es: æœŸæœ›è¼¸å‡º
    - epochs: è¨“ç·´è¼ªæ•¸
    - learning_rate: å­¸ç¿’ç‡
    - print_every: æ¯å¹¾å€‹ epoch æ‰“å°ä¸€æ¬¡é€²åº¦
    - weight_mean, weight_std: ç”¨æ–¼è¨ˆç®—çœŸå¯¦èª¤å·®ï¼ˆå¯é¸ï¼‰
    - save_history: æ˜¯å¦ä¿å­˜è¨“ç·´æ­·å²
    """
    import time
    
    print("\n" + "="*60)
    print("ğŸ§  å»ºç«‹ç¥ç¶“ç¶²çµ¡")
    print("="*60)
    
    # ç¶²çµ¡æ¶æ§‹ï¼šè¼¸å…¥(2) â†’ éš±è—å±¤(8, ReLU) â†’ è¼¸å‡º(1, Linear)
    nn = Network([
        (8, 'relu'),      # éš±è—å±¤ï¼š8 å€‹ç¥ç¶“å…ƒï¼ŒReLU æ¿€æ´»
        (1, 'linear')     # è¼¸å‡ºå±¤ï¼š1 å€‹ç¥ç¶“å…ƒï¼ŒLinear æ¿€æ´»ï¼ˆå›æ­¸ä»»å‹™ï¼‰
    ])
    
    print("ç¶²çµ¡æ¶æ§‹ï¼š")
    print("  è¼¸å…¥å±¤: 2 å€‹ç‰¹å¾µ [æ€§åˆ¥ç·¨ç¢¼, æ¨™æº–åŒ–èº«é«˜]")
    print("  éš±è—å±¤: 8 å€‹ç¥ç¶“å…ƒï¼ˆReLU æ¿€æ´»ï¼‰")
    print("  è¼¸å‡ºå±¤: 1 å€‹ç¥ç¶“å…ƒï¼ˆLinear æ¿€æ´»ï¼‰")
    print(f"\nå­¸ç¿’ç‡: {learning_rate}")
    print(f"è¨“ç·´è¼ªæ•¸: {epochs}")
    
    loss_fn = MSELoss()
    
    # è¨“ç·´æ­·å²è¨˜éŒ„
    history = {
        'epoch': [],
        'loss': [],
        'avg_error_pounds': [] if weight_mean and weight_std else None
    }
    
    print("\n" + "="*60)
    print("ğŸ‹ï¸ é–‹å§‹è¨“ç·´")
    print("="*60)
    
    start_time = time.time()
    
    # è¨“ç·´å¾ªç’°
    for epoch in range(epochs):
        epoch_loss_sum = 0
        epoch_error_sum = 0 if weight_mean and weight_std else None
        
        # éæ­·æ‰€æœ‰è¨“ç·´è³‡æ–™
        for x, e in zip(xs, es):
            # å‰å‘å‚³æ’­
            outputs = nn.forward(x)
            
            # è¨ˆç®—æå¤±
            loss = loss_fn.get_total_loss(outputs, e)
            epoch_loss_sum += loss
            
            # å¦‚æœæä¾›äº†çµ±è¨ˆå€¼ï¼Œè¨ˆç®—çœŸå¯¦èª¤å·®
            if weight_mean and weight_std:
                predicted_weight = outputs[0] * weight_std + weight_mean
                actual_weight = e * weight_std + weight_mean
                error = abs(predicted_weight - actual_weight)
                epoch_error_sum += error
            
            # è¨ˆç®—æ¢¯åº¦
            output_gradients = loss_fn.get_output_gradients(outputs, e)
            
            # åå‘å‚³æ’­
            nn.backward(output_gradients)
            
            # æ›´æ–°æ¬Šé‡
            nn.zero_grad(learning_rate)
        
        # è¨ˆç®—å¹³å‡æå¤±
        avg_loss = epoch_loss_sum / len(xs)
        avg_error = epoch_error_sum / len(xs) if epoch_error_sum is not None else None
        
        # è¨˜éŒ„æ­·å²
        history['epoch'].append(epoch + 1)
        history['loss'].append(avg_loss)
        if avg_error is not None:
            history['avg_error_pounds'].append(avg_error)
        
        # æ‰“å°é€²åº¦
        if (epoch + 1) % print_every == 0 or epoch == 0:
            if avg_error is not None:
                print(f"Epoch {epoch+1:4d}/{epochs}: "
                      f"æå¤±={avg_loss:.6f}, èª¤å·®={avg_error:.2f}ç£…")
            else:
                print(f"Epoch {epoch+1:4d}/{epochs}: å¹³å‡æå¤± = {avg_loss:.6f}")
    
    elapsed_time = time.time() - start_time
    
    print("\nâœ“ è¨“ç·´å®Œæˆï¼")
    print(f"ç¸½è¨“ç·´æ™‚é–“: {elapsed_time:.2f} ç§’")
    print(f"å¹³å‡æ¯å€‹ epoch: {elapsed_time/epochs*1000:.2f} æ¯«ç§’")
    
    # ä¿å­˜è¨“ç·´æ­·å²
    if save_history:
        save_training_history(history, weight_mean, weight_std)
    
    return nn, history


def save_training_history(history, weight_mean, weight_std):
    """
    å°‡è¨“ç·´æ­·å²ä¿å­˜åˆ° CSV æª”æ¡ˆ
    
    åƒæ•¸ï¼š
    - history: è¨“ç·´æ­·å²å­—å…¸
    - weight_mean, weight_std: é«”é‡çµ±è¨ˆå€¼
    """
    print("\n" + "="*60)
    print("ğŸ’¾ ä¿å­˜è¨“ç·´æ­·å²")
    print("="*60)
    
    filename = 'training_history.csv'
    
    with open(filename, 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        
        # å¯«å…¥è¡¨é ­
        if history['avg_error_pounds'] is not None:
            writer.writerow(['Epoch', 'Loss', 'Avg_Error_Pounds'])
            # å¯«å…¥è³‡æ–™
            for i in range(len(history['epoch'])):
                writer.writerow([
                    history['epoch'][i],
                    history['loss'][i],
                    history['avg_error_pounds'][i]
                ])
        else:
            writer.writerow(['Epoch', 'Loss'])
            # å¯«å…¥è³‡æ–™
            for i in range(len(history['epoch'])):
                writer.writerow([
                    history['epoch'][i],
                    history['loss'][i]
                ])
    
    print(f"âœ“ è¨“ç·´æ­·å²å·²ä¿å­˜åˆ°: {filename}")
    print(f"âœ“ å…± {len(history['epoch'])} ç­†è¨˜éŒ„")
    
    # é¡¯ç¤ºçµ±è¨ˆè³‡è¨Š
    analyze_training_history(history)


def analyze_training_history(history):
    """
    åˆ†æè¨“ç·´æ­·å²
    
    åƒæ•¸ï¼š
    - history: è¨“ç·´æ­·å²å­—å…¸
    """
    print("\nğŸ“ˆ è¨“ç·´éç¨‹åˆ†æ:")
    
    # æ‰¾å‡ºæœ€ä½³çµæœ
    best_loss_idx = history['loss'].index(min(history['loss']))
    
    print(f"  æœ€ä½æå¤±: Epoch {history['epoch'][best_loss_idx]} = {history['loss'][best_loss_idx]:.6f}")
    
    if history['avg_error_pounds'] is not None:
        best_error_idx = history['avg_error_pounds'].index(min(history['avg_error_pounds']))
        print(f"  æœ€ä½èª¤å·®: Epoch {history['epoch'][best_error_idx]} = {history['avg_error_pounds'][best_error_idx]:.2f} ç£…")
    
    # è¨ˆç®—æ”¹é€²å¹…åº¦
    initial_loss = history['loss'][0]
    final_loss = history['loss'][-1]
    loss_improvement = (initial_loss - final_loss) / initial_loss * 100
    
    print(f"  æå¤±æ”¹å–„: {initial_loss:.6f} â†’ {final_loss:.6f} ({loss_improvement:.2f}%)")
    
    if history['avg_error_pounds'] is not None:
        initial_error = history['avg_error_pounds'][0]
        final_error = history['avg_error_pounds'][-1]
        error_improvement = (initial_error - final_error) / initial_error * 100
        print(f"  èª¤å·®æ”¹å–„: {initial_error:.2f} â†’ {final_error:.2f} ç£… ({error_improvement:.2f}%)")
    
    # æª¢æŸ¥æ”¶æ–‚æƒ…æ³
    if len(history['loss']) >= 100:
        last_100_losses = history['loss'][-100:]
        loss_mean = sum(last_100_losses) / 100
        loss_std = (sum((l - loss_mean)**2 for l in last_100_losses) / 100) ** 0.5
        
        print(f"\n  æ”¶æ–‚æƒ…æ³ï¼ˆæœ€å¾Œ 100 å€‹ epochï¼‰:")
        print(f"    æå¤±æ¨™æº–å·®: {loss_std:.8f}")
        
        if loss_std < 0.0001:
            print(f"    âœ… æ¨¡å‹å·²å……åˆ†æ”¶æ–‚")
        elif loss_std < 0.001:
            print(f"    âš ï¸ æ¨¡å‹æ¥è¿‘æ”¶æ–‚")
        else:
            print(f"    ğŸ“ˆ æ¨¡å‹ä»åœ¨å­¸ç¿’")


# ============================================================
# ç¬¬äº”æ­¥ï¼šè©•ä¼°æ¨¡å‹
# ============================================================

def evaluate_model(nn, xs, es, weight_mean, weight_std):
    """
    è©•ä¼°æ¨¡å‹æ€§èƒ½
    
    åƒæ•¸ï¼š
    - nn: è¨“ç·´å¥½çš„ç¥ç¶“ç¶²çµ¡
    - xs: æ¸¬è©¦è³‡æ–™è¼¸å…¥
    - es: æ¸¬è©¦è³‡æ–™æœŸæœ›è¼¸å‡º
    - weight_mean, weight_std: ç”¨æ–¼é‚„åŸé«”é‡çš„çµ±è¨ˆå€¼
    
    è¿”å›ï¼š
    - avg_error_pounds: å¹³å‡èª¤å·®ï¼ˆç£…ï¼‰
    """
    print("\n" + "="*60)
    print("ğŸ“Š è©•ä¼°æ¨¡å‹")
    print("="*60)
    
    loss_fn = MSELoss()
    loss_sum = 0
    error_sum_pounds = 0
    
    predictions = []
    actuals = []
    
    for x, e in zip(xs, es):
        # å‰å‘å‚³æ’­
        outputs = nn.forward(x)
        
        # è¨ˆç®—æ¨™æº–åŒ–çš„æå¤±
        loss = loss_fn.get_total_loss(outputs, e)
        loss_sum += loss
        
        # é‚„åŸæˆçœŸå¯¦çš„é«”é‡ï¼ˆç£…ï¼‰
        predicted_weight = outputs[0] * weight_std + weight_mean
        actual_weight = e * weight_std + weight_mean
        
        predictions.append(predicted_weight)
        actuals.append(actual_weight)
        
        # è¨ˆç®—èª¤å·®ï¼ˆç£…ï¼‰
        error = abs(predicted_weight - actual_weight)
        error_sum_pounds += error
    
    # è¨ˆç®—å¹³å‡å€¼
    avg_loss = loss_sum / len(xs)
    avg_error_pounds = error_sum_pounds / len(xs)
    
    print(f"å¹³å‡ MSE æå¤±ï¼ˆæ¨™æº–åŒ–ï¼‰: {avg_loss:.6f}")
    print(f"å¹³å‡çµ•å°èª¤å·®: {avg_error_pounds:.2f} ç£…")
    print(f"ç›¸å°èª¤å·®: {avg_error_pounds / weight_mean * 100:.2f}%")
    
    # é¡¯ç¤ºä¸€äº›é æ¸¬ç¯„ä¾‹
    print("\né æ¸¬ç¯„ä¾‹ï¼ˆå‰ 10 ç­†ï¼‰:")
    print(f"{'åºè™Ÿ':<6} {'é æ¸¬é«”é‡':<12} {'å¯¦éš›é«”é‡':<12} {'èª¤å·®':<10}")
    print("-" * 45)
    for i in range(min(10, len(predictions))):
        error = abs(predictions[i] - actuals[i])
        print(f"{i+1:<6} {predictions[i]:<12.2f} {actuals[i]:<12.2f} {error:<10.2f}")
    
    # åˆ¤æ–·æ˜¯å¦é”æ¨™
    print("\n" + "="*60)
    if avg_error_pounds < 15:
        print(f"ğŸ‰ æ­å–œï¼å¹³å‡èª¤å·® {avg_error_pounds:.2f} ç£… < 15 ç£…ï¼ˆç›®æ¨™é”æˆï¼‰")
    else:
        print(f"ğŸ“ å¹³å‡èª¤å·® {avg_error_pounds:.2f} ç£… > 15 ç£…ï¼ˆéœ€è¦èª¿æ•´ï¼‰")
    print("="*60)
    
    return avg_error_pounds


# ============================================================
# ä¸»ç¨‹å¼
# ============================================================

def main():
    print("\n" + "="*60)
    print("ğŸ¯ Task 1: æ ¹æ“šæ€§åˆ¥å’Œèº«é«˜é æ¸¬é«”é‡ï¼ˆå¢å¼·ç‰ˆï¼‰")
    print("="*60)
    
    # è¼‰å…¥ä¸¦é è™•ç†è³‡æ–™
    xs, es, height_mean, height_std, weight_mean, weight_std = load_and_preprocess_data(
       'C:/Users/user/Downloads/week4/gender-height-weight.csv'
        
    )
    
    # è¨“ç·´æ¨¡å‹ï¼ˆå¢å¼·ç‰ˆ - è¨˜éŒ„æ‰€æœ‰è¨“ç·´éç¨‹ï¼‰
    # å¯ä»¥èª¿æ•´åƒæ•¸ï¼š
    # - epochs: è¨“ç·´è¼ªæ•¸ï¼ˆ500 æˆ– 1000ï¼‰
    # - print_every: æ¯å¹¾å€‹ epoch é¡¯ç¤ºä¸€æ¬¡ï¼ˆ1=å…¨éƒ¨é¡¯ç¤º, 50=æ¯50æ¬¡é¡¯ç¤ºï¼‰
    # - save_history: æ˜¯å¦ä¿å­˜è¨“ç·´æ­·å²åˆ° CSV
    nn, history = train_model(
        xs, es, 
        epochs=1000,           # è¨“ç·´ 1000 è¼ª
        learning_rate=0.01, 
        print_every=100,       # æ¯ 100 å€‹ epoch é¡¯ç¤ºä¸€æ¬¡ï¼ˆé¿å…åˆ·å±ï¼‰
        weight_mean=weight_mean,
        weight_std=weight_std,
        save_history=True      # ä¿å­˜è©³ç´°è¨˜éŒ„åˆ° CSV
    )
    
    # è©•ä¼°æ¨¡å‹
    avg_error = evaluate_model(nn, xs, es, weight_mean, weight_std)
    
    print("\nâœ… ç¨‹å¼åŸ·è¡Œå®Œç•¢ï¼")
    print(f"\nğŸ’¡ æç¤º:")
    print(f"  - è¨“ç·´æ­·å²å·²ä¿å­˜åˆ° training_history.csv")
    print(f"  - å¯ä»¥ç”¨ Excel æˆ–è¨˜äº‹æœ¬æ‰“é–‹æŸ¥çœ‹æ¯å€‹ epoch çš„è©³ç´°æ•¸æ“š")
    print(f"  - ç¸½å…±è¨˜éŒ„äº† {len(history['epoch'])} å€‹ epoch çš„è¨“ç·´éç¨‹")


if __name__ == "__main__":
    main()